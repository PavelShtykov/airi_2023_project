{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch as torch\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import ml_collections\n",
    "from models.ddpm import DDPM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, RandomRotation, RandomVerticalFlip, RandomHorizontalFlip, Normalize, Resize, Compose, CenterCrop, RandomCrop\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms.functional as TVF\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from EMA import EMA\n",
    "\n",
    "def norm_range(x: torch.tensor):\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    x = (x - x_min) / (x_max - x_min) * 255\n",
    "    x = x.to(torch.uint8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_mnist_config():\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # data\n",
    "    data = config.data = ml_collections.ConfigDict()\n",
    "    data.image_size = 64\n",
    "    data.num_channels = 3\n",
    "    data.centered = True\n",
    "    data.batch_size = 256\n",
    "\n",
    "    # model\n",
    "    model = config.model = ml_collections.ConfigDict()\n",
    "    model.ema_rate = 0.99\n",
    "    model.nf = 32\n",
    "    model.ch_mult = (1, 2, 2, 2)\n",
    "    model.num_res_blocks = 3\n",
    "    model.attn_resolutions = (16,)\n",
    "    model.dropout = 0.1\n",
    "    model.resamp_with_conv = True\n",
    "    model.conditional = True\n",
    "    model.nonlinearity = 'swish'\n",
    "\n",
    "    #optim\n",
    "    optim = config.optim = ml_collections.ConfigDict()\n",
    "    optim.grad_clip_norm = 1.0\n",
    "    optim.lr = 3e-4\n",
    "    optim.step_size = 7\n",
    "    optim.gamma = 0.99\n",
    "    optim.weight_decay = 0\n",
    "\n",
    "    # sde\n",
    "    sde = config.sde = ml_collections.ConfigDict()\n",
    "    sde.N = 500\n",
    "    sde.val_samples = 10\n",
    "\n",
    "    config.device = 'cuda:0'\n",
    "    \n",
    "    return config\n",
    "\n",
    "config = create_default_mnist_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoloToPair:\n",
    "    def __init__(self, augment, p=1):\n",
    "        self.augment = augment\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if self.p == 1 or torch.rand(1) < self.p:\n",
    "            args = [self.augment(im) for im in args]\n",
    "            \n",
    "        return args\n",
    "\n",
    "class PairedCompose:\n",
    "    def __init__(self, func_list):\n",
    "        self.func_list = func_list\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        for curr_func in self.func_list:\n",
    "            args = curr_func(*args)\n",
    "            \n",
    "        return args\n",
    "\n",
    "class PairCrop:\n",
    "    def __init__(self, size, p):\n",
    "        self.h, self.w = size\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        if self.p == 1 or torch.rand(1) < self.p:\n",
    "            _, H, W = args[0].shape\n",
    "            assert self.h < H and self.w < W\n",
    "            dH = (torch.rand(1) * (H - self.h)).to(torch.int32)\n",
    "            dW = (torch.rand(1) * (W - self.w)).to(torch.int32)\n",
    "\n",
    "            args = [TVF.crop(img, top=dH, left=dW, height=self.h, width=self.w) for img in args]\n",
    "        \n",
    "        return args\n",
    "            \n",
    "class PairBlur:\n",
    "    def __init__(self, kernel, std_range, p):\n",
    "        self.kernel = kernel\n",
    "        self.std_range = std_range\n",
    "        self.p = p \n",
    "\n",
    "    def __call__(self, img, tar):\n",
    "        if torch.rand(1) < self.p:\n",
    "            img = TVF.gaussian_blur(img, self.kernel, self.std_range)\n",
    "        \n",
    "        return img, tar\n",
    "\n",
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, path, transforms=None, augment=None):\n",
    "        pair = np.load(path)\n",
    "        self.imgs = pair[1]\n",
    "        self.tars = pair[0]\n",
    "\n",
    "        self.imgs = torch.stack([transforms(p) for p in self.imgs])\n",
    "        self.tars = torch.stack([transforms(p) for p in self.tars])\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        tar = self.tars[idx]\n",
    "\n",
    "        if self.augment is not None:\n",
    "            img, tar = self.augment(img, tar)\n",
    "\n",
    "        return img, tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dir = './np_celeba_blur'\n",
    "\n",
    "# train_ds = Pix2PixDataset(\n",
    "#     path=f'{ds_dir}/train.npz',\n",
    "#     transforms=Compose([\n",
    "#         ToTensor(),\n",
    "#         Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     augment=PairedCompose([\n",
    "#         # PairCrop((128, 128), 1),\n",
    "#         # SoloToPair(Resize(size=config.data.image_size)),\n",
    "#         SoloToPair(TVF.vflip, 0.5),\n",
    "#         SoloToPair(TVF.hflip, 0.5),\n",
    "#         # PairBlur(3, [0.1, 1], 0.3)\n",
    "#     ])\n",
    "#     )\n",
    "val_ds = Pix2PixDataset(\n",
    "    path=f'{ds_dir}/val.npz',\n",
    "    transforms=Compose([\n",
    "        ToTensor(),\n",
    "        # CenterCrop(128),\n",
    "        # Resize(size=config.data.image_size),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_pair = train_ds[100]\n",
    "# fig, axes = plt.subplots(1, 2)\n",
    "# axes[0].imshow(TVF.to_pil_image(norm_range(curr_pair[0])))\n",
    "# axes[1].imshow(TVF.to_pil_image(norm_range(curr_pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_ds, batch_size=config.data.batch_size, shuffle=True, drop_last=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFM(pl.LightningModule):\n",
    "    def __init__(self, config, gamma=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.h = 1 / config.sde.N\n",
    "        self.model = DDPM(config)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _base_step(self, batch, batch_idx):\n",
    "        x0, x1 = batch\n",
    "        t = torch.rand(x0.shape[0], device=self.device, dtype=x0.dtype)[:, None, None, None]\n",
    "        t = torch.clip(t, 1e-5, 1 - 1e-5)\n",
    "\n",
    "        xt = t * x1 + (1 - t) * x0\n",
    "\n",
    "        if self.gamma is not None:\n",
    "            ga, dga = self.gamma(t)\n",
    "            z = torch.randn_like(xt)\n",
    "\n",
    "            xt += ga * z\n",
    "            pred = self.model(xt, torch.squeeze(t))\n",
    "            loss = nn.functional.mse_loss(pred, x1 - x0 + dga * z)\n",
    "        else:\n",
    "            pred = self.model(xt, torch.squeeze(t))\n",
    "            loss = nn.functional.mse_loss(pred, x1 - x0)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._base_step(batch, batch_idx)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._base_step(batch, batch_idx)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        img, tar = batch\n",
    "        val_len = self.config.sde.val_samples\n",
    "\n",
    "        shift_idx = 15\n",
    "        sampled = self.predict_step(img[shift_idx:shift_idx+val_len], batch_idx)\n",
    "        stacked = torch.cat([img[shift_idx:shift_idx+val_len], sampled, tar[shift_idx:shift_idx+val_len]], dim=2)\n",
    "        log_imgs = [x for x in stacked]\n",
    "        \n",
    "        self.logger.log_image(\n",
    "            key=\"val_sampled\", \n",
    "            images=log_imgs, \n",
    "            caption=[\"init-sampled-target\"] * len(log_imgs))\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.config.optim.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=self.config.optim.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, \n",
    "            step_size=self.config.optim.step_size,\n",
    "            gamma=self.config.optim.gamma)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        steps = torch.linspace(1e-5, 1 - 1e-5, config.sde.N, device=batch.device, dtype=batch.dtype)[:, None]\n",
    "        x0 = batch\n",
    "\n",
    "        for curr_t in steps:\n",
    "            x0 = x0 + self.h * self.model(x0, curr_t)\n",
    "        \n",
    "        return x0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshtykov-pa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230728_105130-79ru8e40</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shtykov-pa/Flow_matching_ODE/runs/79ru8e40' target=\"_blank\">pretty-bird-1</a></strong> to <a href='https://wandb.ai/shtykov-pa/Flow_matching_ODE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shtykov-pa/Flow_matching_ODE' target=\"_blank\">https://wandb.ai/shtykov-pa/Flow_matching_ODE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shtykov-pa/Flow_matching_ODE/runs/79ru8e40' target=\"_blank\">https://wandb.ai/shtykov-pa/Flow_matching_ODE/runs/79ru8e40</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | DDPM | 2.9 M \n",
      "-------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.663    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da4214e87d480db10c691af1a7d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a63f9a15e4302889d3233b94c6ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlbase/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = CFM(\n",
    "    config, \n",
    "    )\n",
    "wandb_logger = WandbLogger(project='Flow_matching_ODE', log_model='best')\n",
    "ema_callback = EMA(config.model.ema_rate, config.device)\n",
    "# swa_callback = StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=0.8)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gradient_clip_val=config.optim.grad_clip_norm, \n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[ema_callback],\n",
    "    limit_val_batches=1,\n",
    "    check_val_every_n_epoch=3,\n",
    "    # val_check_interval=0.5,\n",
    "    # accumulate_grad_batches=1,\n",
    "    max_epochs=200\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CFM.load_from_checkpoint(\n",
    "    # checkpoint_path='./checkpoints/ode_celeba64_deblur.ckpt',\n",
    "#     config=config\n",
    "# )\n",
    "# model = model.to('cuda:0')\n",
    "# model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     pred = model.predict_step(val_ds[1][1].repeat(10, 1, 1, 1).to('cuda:0'), 1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(3, 4, figsize=(20, 20))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, im in enumerate(pred):\n",
    "#     axes[i].imshow(TVF.to_pil_image(norm_range(unnormalize(im))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
